# pcfg-transformer-learning

This repository contains tools for analyzing transformer models trained on probabilistic context-free grammars (PCFGs).  
It was used to run the experiments in the paper:

> *Unraveling Syntax: How Language Models Learn Context-Free Grammars*

---

## Repository Structure

### Files

- **`def_pcfgs.py`**  
  Contains all PCFG grammars defined for the project.  
  If you wish to add more grammars, extend the `GRAMMARS` dictionary in this file.

- **`generate_pcfg.py`**  
  This is the main script for dataset preparation; generates the tokenizer, training set, and test set for the predefined grammars. The test set size is fixed at 1,000 datapoints, while the training set size is variable and specified by the user. The tokenizers do not have an UNK or PAD token.

- **`train.py`**
    Contains the function `trainer` that trains the transformer model. 
    It supports two modes:

    - **Training from scratch**:  
        Starts a fresh training run, with results stored in the `new` directory.

    - **Continuing from a checkpoint**: Loads the model from an existing checkpoint and continues training. All pre-existing checkpoints from the source directory are copied into the new target directory, and results are stored under `continued`.
  
    This directory structure ensures a clear distinction between newly initialized models and those extended from checkpoints.  
    If the same model type is trained on the same PCFG and start symbol, the corresponding folder will be overwritten!

- **`analysis_hierarchy.py`** 
  This is the main script to analyze the models *after* being trained on a grammar. 
  To analyze a specific subgrammar, provide its name; otherwise, set `--subgrammar` to the name of the full grammar. It estimates the KL divergence across training epochs and plots it.
  If `create_table` is passed, it stores the final epoch value for each seed in `kl_table.csv`.  

- **`activation_space.py`**
    To analyze the cosine similarity of models, supports both within-set and cross-set comparisons, aggregates results across seeds and can output per-layer heatmaps. 
    To analyze within-set comparison, the `.txt` file should contain a single block of sequences, one sequence per line. For cross-set comparison, the `.txt` file should contain **two blocks** of sequences, separated by an **empty line** to ensure correct parsing.

- **`depth_recursion_exp.py`**
    This script trains and evaluates the transformer **TwoLayer_LARGE** model on the **NestedParentheses** grammar. It generates synthetic sequences, compares model-predicted logits with handcrafted ground-truth distributions, and visualizes prediction errors across depths and random seeds. The pipeline includes tokenizer generation, training the model on multiple seeds, and systematic analysis of different input cases and prefixes.

- **`generate_arithmetic_expr.ipynb`**  
  A Jupyter Notebook that can be used to generate random nested arithmetic expressions,  
  up to a maximum nesting depth and number of terms.


---

## Usage

1. **Define or extend grammars** in `def_pcfgs.py` if needed.
2. **Generate data** with `generate_pcfg.py` for the grammar of interest.
3. **Optionally experiment** with arithmetic datasets using `math.ipynb`.


The Transformer architecture is based on the GPT-2 architecture, but then scaled down, since the languages generated by the PCFGs are more compact than natural language. 