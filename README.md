# pcfg-transformer-learning

This repository contains tools for analyzing transformer models trained on probabilistic context-free grammars (PCFGs).  
It was used to run the experiments in the paper:

> *Unraveling Syntax: How Language Models Learn Context-Free Grammars*

---

## Repository Structure

### Files

- **`def_pcfgs.py`**  
  Contains all PCFG grammars defined for the project.  
  If you wish to add more grammars, extend the `GRAMMARS` dictionary in this file.

- **`generate_pcfg.py`**  
  This is the main script for dataset preparation; generates the tokenizer, training set, and test set for the predefined grammars. The test set size is fixed at 1,000 datapoints, while the training set size is variable and specified by the user. The tokenizers do not have an UNK or PAD token.

- **`train.py`**
    Contains the function `trainer` that trains the transformer model. 
    It supports two modes:

    - **Training from scratch**:  
        Starts a fresh training run, with results stored in the `new` directory.

    - **Continuing from a checkpoint**: Loads the model from an existing checkpoint and continues training. All pre-existing checkpoints from the source directory are copied into the new target directory, and results are stored under `continued`.
  
    This directory structure ensures a clear distinction between newly initialized models and those extended from checkpoints.  
    If the same model type is trained on the same PCFG and start symbol, the corresponding folder will be overwritten!

- **`activation_space.py`**
    To analyze the cosine similarity of models, supports both within-set and cross-set comparisons, aggregates results across seeds and can output per-layer heatmaps. 
    To analyze within-set comparison, the `.txt` file should contain a single block of sequences, one sequence per line. For cross-set comparison, the `.txt` file should contain **two blocks** of sequences, separated by an **empty line** to ensure correct parsing.

- **`generate_arithmetic_expr.ipynb`**  
  A Jupyter Notebook that can be used to generate random nested arithmetic expressions,  
  up to a maximum nesting depth and number of terms.


---

## Usage

1. **Define or extend grammars** in `def_pcfgs.py` if needed.
2. **Generate data** with `generate_pcfg.py` for the grammar of interest.
3. **Optionally experiment** with arithmetic datasets using `math.ipynb`.


The Transformer architecture is based on the GPT-2 architecture, but then scaled down, since the languages generated by the PCFGs are more compact than natural language. 